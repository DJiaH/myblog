---
title: 机器学习-吴恩达
date: 2021-08-30 17:28:02
tags:
	- 机器学习
	- 吴恩达
categories:
	- 机器学习
mathjax: true
description: 刚接触机器学习发现有些东西没想象中的那么难，当然也不简单
---

# 初学机器学习
## 1监督学习(Supervised learning)
&emsp; &emsp;对于dataset中的每个样本，进行预测，并得到正确答案
&emsp; &emsp;回归问题（regression）：预测一组连续值的输出
&emsp; &emsp;分类问题（classification）：预测一组离散值的输出（0/1）
## 2无监督学习
&emsp; &emsp;聚类（clustering），将一组无特征值的数据进行划分

----
# Linear regression with one variable(单变量线性回归)
## 3模型描述
&emsp; &emsp;假设一个函数(Hypothesis):这个函数将会预测输入的数据
## 4代价函数（损失函数）
&emsp; &emsp;Hypothesis(假设函数):  $$ h_\Theta(x) = \Theta_0 +\Theta_1x $$
&emsp; &emsp;Parameters（参数）:  $$ \Theta_0 ,\Theta_1 $$
&emsp; &emsp;Cost Function（代价函数）: $$ J(\Theta_0,\Theta_1) =\frac{1}{2m} \sum_{i=1}^m(h_\Theta(x_i)-y_i)^2$$
&emsp; &emsp;Goal（目标）: $minimize_{\Theta_0,\Theta_1} J(\Theta_0,\Theta_1)$
平方差对于大多数问题，特别是回归问题都是一个合理的选择。当$J(\Theta_0,\Theta_1)$取值最小的时候，就是函数拟合最好的时候。
&emsp; &emsp;由于平方之后求导会多出两个常量2，所以代价函数乘以$\frac{1}{2}$正好可以消掉
## 5梯度下降
&emsp; &emsp;损失函数：$J(\Theta_0,\Theta_1)$

&emsp; &emsp;目标函数：$min_{\Theta_0,\Theta_1} J(\Theta_0,\Theta_1)$思路：
 - 随机初始化$\Theta_0,\Theta_1$
 - 持续改变$\Theta_0,\Theta_1$，减少$J(\Theta_0,\Theta_1)$直到达到一个最小值
&emsp; &emsp;使用梯度下降的时候，每次计算都朝着下降速度最快的方向，但是不同的初始值可能导致最后走向不同的局部最低点。局部最低点可能不止一个。
**梯度下降算法（Gradient descent algorithm）**
反复知道收敛（convergence）：
![在这里插入图片描述](https://img-blog.csdnimg.cn/15d79a638cee474d83748b2c6841847c.png)
**注意：同步跟新**
![在这里插入图片描述](https://img-blog.csdnimg.cn/18da1d5fff4b4f72a965221e3ed92686.png)
`：=`为赋值符号
$\alpha$为学习率
## 6梯度下降知识点总结
&emsp; &emsp;导数项决定斜率(slope)即下降方向始终朝向最小值。
&emsp; &emsp;如果当$\Theta$已经在局部最小，则此处斜率为0，$\Theta$将不会再改变。

&emsp; &emsp;如果学习率太小，会导致下降速度很慢，同时也可能陷入局部最优值(local optima)无法出来。
&emsp; &emsp;如果学习率太大，可能会跨过最小值，反复横跳，导致无法收敛(converge)甚至发散(diverge)
&emsp; &emsp;随着逼近局部最低点，斜率减小，此时每次更新的幅度也将随之自动变得越来越小，所以没必要改变$\alpha$
## 7线性回归梯度下降
![在这里插入图片描述](https://img-blog.csdnimg.cn/f4dfd5b37dc04f089b755cf4fb8fc0a7.png)
&emsp; &emsp;分别求导:
![在这里插入图片描述](https://img-blog.csdnimg.cn/5031ddc2aabb42acacee9e780c317b27.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_9,color_FFFFFF,t_70,g_se,x_16)
&emsp; &emsp;凸函数(convex function)只有一个局部最优解
&emsp; &emsp;"Batch"梯度下降：意味着每一步下降都会使用整个训练集的样本

----
# Linear Algebra review（线性代数回顾）
## 8矩阵和向量
&emsp; &emsp;矩阵的维数:行数×列数，(用大写字母表示矩阵)
&emsp; &emsp;$A_{ij}$=在矩阵里第i行第j列的元素

&emsp; &emsp;向量：n×1的矩阵，(用小写字母表示向量)
&emsp; &emsp;$y_i$=第i个元素
## 9加法和标量乘法
&emsp; &emsp;矩阵的加法运算
&emsp; &emsp;矩阵的标量乘法运算
## 10矩阵和向量的乘法
&emsp; &emsp;将矩阵每一行的元素分别与向量的每一个元素相乘并加起来。
**注意**：矩阵和向量相乘必须满足矩阵的列数与向量的维度相等
## 11矩阵与矩阵相乘
&emsp; &emsp;m×n的矩阵和n×o的矩阵得到m×o的矩阵
## 12矩阵乘法特征
&emsp; &emsp;A×B不等于B×A，不满足交换律
&emsp; &emsp;A×B×C=A×(B×C)，满足结合率
&emsp; &emsp;单位矩阵(Identity Matrix):$I$or$I_{n×n}$
## 13逆和转置(Inverse,transpose)
&emsp; &emsp;$A$是一个n×n的方阵
&emsp; &emsp;$AA^{-1}=A^{-1}A$=$I$

&emsp; &emsp;**注意**:只有方阵才可能有逆矩阵，没有逆矩阵的矩阵称为奇异矩阵或退化矩阵(singular,degenerate)
&emsp; &emsp;$A$是一个m×n的矩阵，让$B=A^T$,那么$B$就是一个n×m的矩阵。$B_{ij}=A_{ji}$.
____
# Linear Regression with multiple variables(多元线性回归)
## 14多特征
&emsp; &emsp;$x^{(i)}$:第i个训练数据
&emsp; &emsp;$x^{(i)}_j$:第i个训练数据中的第j个特征值（第i行第j列）
&emsp; &emsp;假设函数：
$$h_\Theta(x)=\Theta_0+\Theta_1x_1+\Theta_2x_2+...+\Theta_nx_n$$
&emsp; &emsp;对于每个数据样例都有一个$x^{(i)}_0=1$
$$h_\Theta(x)=\Theta_0x_0+\Theta_1x_1+\Theta_2x_2+...+\Theta_nx_n$$
## 15多元梯度下降法
![在这里插入图片描述](https://img-blog.csdnimg.cn/2cf84c79b8554946bc9035f4b2d69cd5.png)
&emsp; &emsp;因为$x^{(i)}_0=1$,所以仍与之前的一元算法等效
## 16梯度下降法演练1：特征缩放
&emsp; &emsp;如果不同的特征值在相近的范围，这样梯度下降法就能更快的收敛。
&emsp; &emsp;通常我们在进行特征缩放的时候将范围大致限定在$-1\leq{x_i}\leq1$范围内。
**均值归一化（mean normalization）**
&emsp; &emsp;将$x_i$替换为$x_i-u_i$以使得特征值有接近于0的均值。但是不要将其应用到$x_0$上，因为$x_0$恒为1，不可能有为0到平均值。
通常使用如下式子：
$$x_i:= \frac{x_i-u_i}{range_i}$$
&emsp; &emsp;注：
&emsp; &emsp;$x_i$:第i个特征值
&emsp; &emsp;$u_i$:第i个特征值的均值
&emsp; &emsp;$range_i$第i个特征值的取值范围的长度（最大值减去最小值）
&emsp; &emsp;缩放不用特别精准，目的在于加快收敛速度
## 17多元梯度下降法演练2-学习率
&emsp; &emsp;通过绘制迭代次数与最小损失值($min_\theta J(\theta)$)的函数曲线，可以判断梯度下降算法是否正常运行。当曲线趋近于平坦的时候就代表差不多收敛了
![在这里插入图片描述](https://img-blog.csdnimg.cn/5b01b57d2aca48408d4964e09f99bbfb.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_11,color_FFFFFF,t_70,g_se,x_16)
&emsp; &emsp;同样也可以使用自动检测算法通过判断在一次迭代后$J(\theta)$是否小于某一阈值$\varepsilon$来判断是否收敛。但是阈值的取值通常难以确定，所以还是直接绘图观察比较好。

&emsp; &emsp;如果观察到$J(\theta)$曲线在上升或者在上下震荡，通常的办法就是降低学习率$\alpha$.

&emsp; &emsp;在选取$\alpha$的时候，我们从..0.001,0.01,0.1,1,...每隔10倍取值尝试，然后绘图观察，选取一个下降最快的$\alpha$

